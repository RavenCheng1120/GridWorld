{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Prediction\n",
    "## 1. First-visit Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_world\n",
    "from grid_world import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "grid = standard_grid()\n",
    "\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "}\n",
    "\n",
    "print(\"Policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(grid, policy):\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "        \n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        if first:      #跳過第一個(最後的terminal state)\n",
    "            first = False\n",
    "        else:\n",
    "            states_and_returns.append((s, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_and_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_and_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從隨機開始點，玩一次遊戲：\n",
      "[((0, 0), 0.81), ((0, 1), 0.9), ((0, 2), 1.0)]\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# initialize V(s) and returns\n",
    "V = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions:\n",
    "        returns[s] = []\n",
    "    else:\n",
    "        # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "        \n",
    "states_and_returns = play_game(grid, policy)\n",
    "print(\"從隨機開始點，玩一次遊戲：\")\n",
    "print(states_and_returns)\n",
    "\n",
    "for t in range(100):\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()   #創建一個無序不重複元素集\n",
    "    for s, G in states_and_returns:\n",
    "    # check if we have already seen s\n",
    "    # called \"first-visit\" MC policy evaluation\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s] = np.mean(returns[s])    # Returns the average of the array elements\n",
    "            seen_states.add(s)\n",
    "\n",
    "print(\"\\nvalues:\")\n",
    "print_values(V, grid)\n",
    "print(\"\\npolicy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筆記：\n",
    "蒙地卡羅預測是在沒有完整環境資訊的情況下使用。    \n",
    "First-visit MC prediction 是在固定的policy之下，找到各個state的value。    \n",
    "1.先選定隨機起始點，遊玩一次地圖，走到終點算是遊戲結束，此為一個episode。   \n",
    "2.從終點回推各state會得到的G值，把對應state和return(G值)記錄下來。   \n",
    "3.將這個episode中各state第一次出現時的return存起來。   \n",
    "4.重複上述動作許多次，最後將每個state存起來的很多return值平均，就是value值。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "---------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n",
      "\n",
      "Policy:\n",
      "---------------------------\n",
      "  L  |  U  |  U  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  D  |\n"
     ]
    }
   ],
   "source": [
    "grid = negative_grid(step_cost=-0.9)\n",
    "\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "# initialize a random policy\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    \n",
    "print(\"\\nPolicy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "\n",
    "    # reset game to start at a random position\n",
    "    # we need to do this if we have a deterministic policy\n",
    "    # we would never end up at certain states, but we still want to measure their value\n",
    "    # this is called the \"exploring starts\" method\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "\n",
    "    # be aware of the timing\n",
    "    # each triple is s(t), a(t), r(t)\n",
    "    # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    seen_states = set()\n",
    "    seen_states.add(grid.current_state())\n",
    "    num_steps = 0\n",
    "    while True:\n",
    "        r = grid.move(a)\n",
    "        num_steps += 1\n",
    "        s = grid.current_state()\n",
    "\n",
    "        if s in seen_states:\n",
    "            # hack so that we don't end up in an infinitely long episode\n",
    "            # bumping into the wall repeatedly\n",
    "            # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
    "            #   reward = -10\n",
    "            # else:\n",
    "            #   reward = falls off by 1 / num_steps\n",
    "            reward = -10. / num_steps\n",
    "            states_actions_rewards.append((s, None, reward))\n",
    "            break\n",
    "        elif grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = policy[s]\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "        seen_states.add(s)\n",
    "\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    # put this into a function since we are using it so often\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "\n",
      "final values:\n",
      "---------------------------\n",
      "-1.91|-0.74| 1.00| 0.00|\n",
      "---------------------------\n",
      "-2.56| 0.00|-0.70| 0.00|\n",
      "---------------------------\n",
      "-3.07|-2.71|-1.66|-1.00|\n"
     ]
    }
   ],
   "source": [
    "Q = {}\n",
    "returns = {}     # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "\n",
    "for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a] = 0   # needs to be initialized to something so we can argmax it\n",
    "            returns[(s,a)] = []\n",
    "    else:                 # terminal state or state we can't otherwise get to\n",
    "        pass\n",
    "\n",
    "# repeat until convergence\n",
    "deltas = []\n",
    "for t in range(2000):\n",
    "    if t % 100 == 0:\n",
    "        print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = es_play_game(grid, policy)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        sa = (s, a)\n",
    "        if sa not in seen_state_action_pairs:\n",
    "            old_q = Q[s][a]\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a] = np.mean(returns[sa])\n",
    "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "            seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # update policy\n",
    "    for s in policy.keys():\n",
    "        policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "plt.plot(deltas)\n",
    "plt.show()\n",
    "\n",
    "print(\"final policy:\")\n",
    "print_policy(policy, grid)\n",
    "\n",
    "# find V\n",
    "V = {}\n",
    "for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"\\nfinal values:\")\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筆記：\n",
    "policy是隨機決定，因此會有走向牆壁或無法行走的方塊的情況，為了避免無限迴圈，在玩遊戲時有防禦機制(再度走到同一格時，reward變很低)。\n",
    "跟上一個First-visit Monte Carlo程式碼很像，不同處在於加入action影響return，且需要找到最佳policy。\n",
    "要通過max_dict(d)找到每個state在該v[s]之下，最佳的前進方向(policy)。循環多次(2000次)後，就能收斂到最佳解法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo with out Exploring Start(on-policy first-visit MC)\n",
    "![title](image01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    # choose given a with probability 1 - eps + eps/4\n",
    "    # choose some other a' != a with probability eps/4\n",
    "    p = np.random.random()\n",
    "    # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "    #   return a\n",
    "    # else:\n",
    "    #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    #   tmp.remove(a)\n",
    "    #   return np.random.choice(tmp)\n",
    "    #\n",
    "    # this is equivalent to the above\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # in this version we will NOT use \"exploring starts\" method\n",
    "    # instead we will explore using an epsilon-soft policy\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    a = random_action(policy[s])\n",
    "\n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    while True:\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = random_action(policy[s]) # the next state is stochastic\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  D  |  L  |  R  |     |\n",
      "---------------------------\n",
      "  L  |     |  L  |     |\n",
      "---------------------------\n",
      "  D  |  U  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "print(\"\\npolicy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final values:\n",
      "---------------------------\n",
      " 0.58| 0.77| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.40| 0.00| 0.77| 0.00|\n",
      "---------------------------\n",
      " 0.25| 0.11|-0.06|-0.23|\n",
      "\n",
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a] = 0\n",
    "            returns[(s,a)] = []\n",
    "    else:\n",
    "        # terminal state or state we can't otherwise get to\n",
    "        pass\n",
    "        \n",
    "# repeat until convergence\n",
    "deltas = []\n",
    "for t in range(5000):\n",
    "    if t % 1000 == 0:\n",
    "        print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = on_play_game(grid, policy)\n",
    "\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        sa = (s, a)\n",
    "        if sa not in seen_state_action_pairs:\n",
    "            old_q = Q[s][a]\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a] = np.mean(returns[sa])\n",
    "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "            seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "    for s in policy.keys():\n",
    "        a, _ = max_dict(Q[s])\n",
    "        policy[s] = a\n",
    "\n",
    "plt.plot(deltas)\n",
    "plt.show()\n",
    "\n",
    "# find the optimal state-value function\n",
    "# V(s) = max[a]{ Q(s,a) }\n",
    "V = {}\n",
    "for s in policy.keys():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"\\nfinal values:\")\n",
    "print_values(V, grid)\n",
    "print(\"\\nfinal policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筆記：\n",
    "on-policy不再使用exploring start。    \n",
    "先初始一個隨機的policy，他每次固定從一個起點出發(2,0)，當前進時，有一定機率會往別的方向走。     \n",
    "當下一步是走向迷宮外或是不能走的格子時，他會一直在原地不動，直到觸發隨機往別的方向走，才能離開那個撞牆迴圈。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
